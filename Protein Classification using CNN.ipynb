{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Protein Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proteins are “the doers” in the human cell, executing many functions that together enable life. Historically, classification of proteins has been limited to single patterns in one or a few cell types, but in order to fully understand the complexity of the human cell, models must classify mixed patterns across a range of different human cells.\n",
    "\n",
    "Images visualizing proteins in cells are commonly used for biomedical research, and these cells could hold the key for the next breakthrough in medicine. However, thanks to advances in high-throughput microscopy, these images are generated at a far greater pace than what can be manually evaluated. Therefore, the need is greater than ever for automating biomedical image analysis to accelerate the understanding of human cells and disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Encoding of binary target labels out of the given multilabel list per image.\n",
    "2. Visualization of target protein distribution in the train set.\n",
    "3. A simple image generator that yields images of a target-protein-wishlist. Each sample that has at least one match with this list is returned.\n",
    "4. Validation\n",
    "5. A baseline model built with Keras\n",
    "   - A modelparameter class that holds all parameters that are necessary to build the model, to load the data and to preprocess the images.\n",
    "   - An image preprocessor that rescales, reshapes and normalizes the images for feeding into the model.\n",
    "   - A data generator that can be used with CPU/GPU computing to perform training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predict various protein structures in cellular images.\n",
    "- There are 28 different target proteins.\n",
    "- Multiple proteins can be present in one image.\n",
    "- 27 different cell types of highly different morphology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelSettings:\n",
    "    \n",
    "    def __init__(self, fit_baseline=False,\n",
    "                 fit_improved_baseline=False,\n",
    "                 fit_improved_higher_batchsize=False,\n",
    "                 fit_improved_without_dropout=False):\n",
    "        self.fit_baseline = fit_baseline\n",
    "        self.fit_improved_baseline = fit_improved_baseline\n",
    "        self.fit_improved_higher_batchsize = fit_improved_higher_batchsize\n",
    "        self.fit_improved_without_dropout = fit_improved_without_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelsettings = KernelSettings(fit_baseline=False,\n",
    "                                fit_improved_baseline=False,\n",
    "                                fit_improved_higher_batchsize=False,\n",
    "                                fit_improved_without_dropout=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Packages & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"G:\\DeepLearning\\CSE465\\input\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(\"G:/DeepLearning/CSE465/input/train.csv\") #Loading data from local machine\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting test names from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"G:/DeepLearning/CSE465/input/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"G:/DeepLearning/CSE465/input/sample_submission.csv\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = submission.Id.values\n",
    "print(len(test_names))\n",
    "print(test_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 11702 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = {\n",
    "    0:  \"Nucleoplasm\",  \n",
    "    1:  \"Nuclear membrane\",   \n",
    "    2:  \"Nucleoli\",   \n",
    "    3:  \"Nucleoli fibrillar center\",   \n",
    "    4:  \"Nuclear speckles\",\n",
    "    5:  \"Nuclear bodies\",   \n",
    "    6:  \"Endoplasmic reticulum\",   \n",
    "    7:  \"Golgi apparatus\",   \n",
    "    8:  \"Peroxisomes\",   \n",
    "    9:  \"Endosomes\",   \n",
    "    10:  \"Lysosomes\",   \n",
    "    11:  \"Intermediate filaments\",   \n",
    "    12:  \"Actin filaments\",   \n",
    "    13:  \"Focal adhesion sites\",   \n",
    "    14:  \"Microtubules\",   \n",
    "    15:  \"Microtubule ends\",   \n",
    "    16:  \"Cytokinetic bridge\",   \n",
    "    17:  \"Mitotic spindle\",   \n",
    "    18:  \"Microtubule organizing center\",   \n",
    "    19:  \"Centrosome\",   \n",
    "    20:  \"Lipid droplets\",   \n",
    "    21:  \"Plasma membrane\",   \n",
    "    22:  \"Cell junctions\",   \n",
    "    23:  \"Mitochondria\",   \n",
    "    24:  \"Aggresome\",   \n",
    "    25:  \"Cytosol\",   \n",
    "    26:  \"Cytoplasmic bodies\",   \n",
    "    27:  \"Rods & rings\"\n",
    "}\n",
    "\n",
    "reverse_train_labels = dict((v,k) for k,v in label_names.items())\n",
    "\n",
    "def fill_targets(row):\n",
    "    row.Target = np.array(row.Target.split(\" \")).astype(np.int)\n",
    "    for num in row.Target:\n",
    "        name = label_names[int(num)]\n",
    "        row.loc[name] = 1\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Train Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in label_names.keys():\n",
    "    train_labels[label_names[key]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.apply(fill_targets, axis=1)\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can work with binary target values. \n",
    "Then create a DataFrame for the Test IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = pd.DataFrame(data=test_names, columns=[\"Id\"])\n",
    "for col in train_labels.columns.values:\n",
    "    if col != \"Id\":\n",
    "        test_labels[col] = 0\n",
    "test_labels.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not made any predictions and all the entities of the IDs are filled with 0's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Proteins that occur most times in the train images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=0).sort_values(ascending=False)\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.barplot(y=target_counts.index.values, x=target_counts.values, order=target_counts.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summary of EDA\n",
    "- Most common protein structures are coarse grained cellular components like the Plasma Membrane, Cytosol & the Nucleus\n",
    "- Components like Lipid Droplets, Perioxisomes, Endosomes, Lysosomes, Microtubule Ends, Rods & Rings are very rare. \n",
    "- Cosequently accuracy is not very right for measuring the performance and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Common targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[\"number_of_targets\"] = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=1)\n",
    "count_perc = np.round(100 * train_labels[\"number_of_targets\"].value_counts() / train_labels.shape[0], 2)\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(x=count_perc.index.values, y=count_perc.values, palette=\"Reds\")\n",
    "plt.xlabel(\"Number of targets per image\")\n",
    "plt.ylabel(\"% of train data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most train images have only 1 or 2 target labels.\n",
    "- More than 3 targets are rare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(train_labels[train_labels.number_of_targets>1].drop(\n",
    "    [\"Id\", \"Target\", \"number_of_targets\"],axis=1\n",
    ").corr(), cmap=\"RdYlBu\", vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "- Very slight correlations.\n",
    "- Endosomes & Lysosomes often occur together and sometimes can be located the endoplasmic reticulum.\n",
    "- Mitotic Spindle often comes together with the Cytokinetic bridge. Consequently we find a positive correlation between these targets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways how special and Rare targets are grouped. \n",
    "\n",
    "\n",
    "Lysosomes and Endosomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_counts(special_target, labels):\n",
    "    counts = labels[labels[special_target] == 1].drop(\n",
    "        [\"Id\", \"Target\", \"number_of_targets\"],axis=1\n",
    "    ).sum(axis=0)\n",
    "    counts = counts[counts > 0]\n",
    "    counts = counts.sort_values()\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyso_endo_counts = find_counts(\"Lysosomes\", train_labels)\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "sns.barplot(x=lyso_endo_counts.index.values, y=lyso_endo_counts.values, palette=\"Blues\")\n",
    "plt.ylabel(\"Counts in train data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rods & Rings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rod_rings_counts = find_counts(\"Rods & rings\", train_labels)\n",
    "plt.figure(figsize=(15,3))\n",
    "sns.barplot(x=rod_rings_counts.index.values, y=rod_rings_counts.values, palette=\"Greens\")\n",
    "plt.ylabel(\"Counts in train data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perioxisomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peroxi_counts = find_counts(\"Peroxisomes\", train_labels)\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "sns.barplot(x=peroxi_counts.index.values, y=peroxi_counts.values, palette=\"Reds\")\n",
    "plt.ylabel(\"Counts in train data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Microtubule Ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tubeends_counts = find_counts(\"Microtubule ends\", train_labels)\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "sns.barplot(x=tubeends_counts.index.values, y=tubeends_counts.values, palette=\"Purples\")\n",
    "plt.ylabel(\"Counts in train data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuclear Speckles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclear_speckles_counts = find_counts(\"Nuclear speckles\", train_labels)\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "sns.barplot(x=nuclear_speckles_counts.index.values, y=nuclear_speckles_counts.values, palette=\"Oranges\")\n",
    "plt.xticks(rotation=\"70\")\n",
    "plt.ylabel(\"Counts in train data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take-away\n",
    "\n",
    "- With very seldom targets we find some kind of grouping with other targets that reveal where the protein structure seems to be located.\n",
    "- For example, the rods and rings have something to do with the nucleus whereas peroxisomes may be located in the nucleus as well as in the cytosol.\n",
    "- Perhaps this patterns might help to build a more robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do the images look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "files = listdir(\"G:/DeepLearning/CSE465/input/train\")\n",
    "for n in range(10):\n",
    "    print(files[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Each image is actually splitted into 4 different image files.\n",
    "2. These 4 files correspond to 4 different filter:\n",
    "    - a green filter for the target protein structure of interest\n",
    "    - blue landmark filter for the nucleus\n",
    "    - red landmark filter for microtubules\n",
    "    - yellow landmark filter for the endoplasmatic reticulum\n",
    "3. Each image is of size 512 x 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the number of files divided by 4 yields the number of target samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files) / 4 == train_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do images of specific targets look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"G:/DeepLearning/CSE465/input/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(basepath, image_id):\n",
    "    images = np.zeros(shape=(4,512,512))\n",
    "    images[0,:,:] = imread(basepath + image_id + \"_green\" + \".png\")\n",
    "    images[1,:,:] = imread(basepath + image_id + \"_red\" + \".png\")\n",
    "    images[2,:,:] = imread(basepath + image_id + \"_blue\" + \".png\")\n",
    "    images[3,:,:] = imread(basepath + image_id + \"_yellow\" + \".png\")\n",
    "    return images\n",
    "\n",
    "def make_image_row(image, subax, title):\n",
    "    subax[0].imshow(image[0], cmap=\"Greens\")\n",
    "    subax[1].imshow(image[1], cmap=\"Reds\")\n",
    "    subax[1].set_title(\"stained microtubules\")\n",
    "    subax[2].imshow(image[2], cmap=\"Blues\")\n",
    "    subax[2].set_title(\"stained nucleus\")\n",
    "    subax[3].imshow(image[3], cmap=\"Oranges\")\n",
    "    subax[3].set_title(\"stained endoplasmatic reticulum\")\n",
    "    subax[0].set_title(title)\n",
    "    return subax\n",
    "\n",
    "def make_title(file_id):\n",
    "    file_targets = train_labels.loc[train_labels.Id==file_id, \"Target\"].values[0]\n",
    "    title = \" - \"\n",
    "    for n in file_targets:\n",
    "        title += label_names[n] + \" - \"\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetGroupIterator:\n",
    "    \n",
    "    def __init__(self, target_names, batch_size, basepath):\n",
    "        self.target_names = target_names\n",
    "        self.target_list = [reverse_train_labels[key] for key in target_names]\n",
    "        self.batch_shape = (batch_size, 4, 512, 512)\n",
    "        self.basepath = basepath\n",
    "    \n",
    "    def find_matching_data_entries(self):\n",
    "        train_labels[\"check_col\"] = train_labels.Target.apply(\n",
    "            lambda l: self.check_subset(l)\n",
    "        )\n",
    "        self.images_identifier = train_labels[train_labels.check_col==1].Id.values\n",
    "        train_labels.drop(\"check_col\", axis=1, inplace=True)\n",
    "    \n",
    "    def check_subset(self, targets):\n",
    "        return np.where(set(targets).issubset(set(self.target_list)), 1, 0)\n",
    "    \n",
    "    def get_loader(self):\n",
    "        filenames = []\n",
    "        idx = 0\n",
    "        images = np.zeros(self.batch_shape)\n",
    "        for image_id in self.images_identifier:\n",
    "            images[idx,:,:,:] = load_image(self.basepath, image_id)\n",
    "            filenames.append(image_id)\n",
    "            idx += 1\n",
    "            if idx == self.batch_shape[0]:\n",
    "                yield filenames, images\n",
    "                filenames = []\n",
    "                images = np.zeros(self.batch_shape)\n",
    "                idx = 0\n",
    "        if idx > 0:\n",
    "            yield filenames, images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see images that contain the protein structures lysosomes or endosomes as we have dicided to choose these two. Set target values of our choice and the target group iterator will collect all images that are subset of our choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_choice = [\"Lysosomes\", \"Endosomes\"]\n",
    "your_batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageloader = TargetGroupIterator(your_choice, your_batch_size, train_path)\n",
    "imageloader.find_matching_data_entries()\n",
    "iterator = imageloader.get_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids, images = next(iterator)\n",
    "\n",
    "fig, ax = plt.subplots(len(file_ids),4,figsize=(20,5*len(file_ids)))\n",
    "if ax.shape == (4,):\n",
    "    ax = ax.reshape(1,-1)\n",
    "for n in range(len(file_ids)):\n",
    "    make_image_row(images[n], ax[n], make_title(file_ids[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking at the images we can already have some assumptions:\n",
    "  - Staining target proteins with green channel was not a success. The images differ in their intensities and the target proteins are not always located the same way. \n",
    "  - Red channel had some morphological differences.This is just an assumption but perhaps one could use the red channel information to reveal cell types. \n",
    "- Sometimes the whole image is covered with cells and sometimes there are only a few. In addition we can see that some images have higher values whereas others are sallow with overall low values. If we like to detect same targets out of different bright images, this will cause problems as we expect them to be in a similar value range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the performance of our model we will use k-fold cross validation. The train data is splitted into k chunks and each chunk is used once for testing the prediction performance whereas the others are used for training.\n",
    "We will repeat the K-Fold several times and look at scoring distributions in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = listdir(\"G:/DeepLearning/CSE465/input/train\")\n",
    "test_files = listdir(\"G:/DeepLearning/CSE465/input/test\")\n",
    "percentage = np.round(len(test_files) / len(train_files) * 100)\n",
    "\n",
    "print(\"The test set size turns out to be {}% compared to the train set.\".format(percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data is 38 % of size compared to the train set it makes sense to use 3-Fold cross validation where the test set is 33 % of size compared to the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "splitter = RepeatedKFold(n_splits=3, n_repeats=1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This splitter is now a generator. Hence calling split method it will yield one Fold of the repeated K-Folds. Consequently if we choose n_repeats=2 we will end up with 6 Folds in total: 3 Folds for the first cross validation and again 3 Folds for the repeated cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = []\n",
    "\n",
    "for train_idx, test_idx in splitter.split(train_labels.index.values):\n",
    "    partition = {}\n",
    "    partition[\"train\"] = train_labels.Id.values[train_idx]\n",
    "    partition[\"validation\"] = train_labels.Id.values[test_idx]\n",
    "    partitions.append(partition)\n",
    "    print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    print(\"TRAIN:\", len(train_idx), \"TEST:\", len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions[0][\"train\"][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ideas\n",
    "We will be using Deep Learning library Keras. This makes it easier to build and train Neural Networks.\n",
    "- Using only green channel for the images to make it simpler. \n",
    "- Using generators to only load data images of the batch and not all at once. Using keras fit_generator, evaluate_generator and predict_generator we can directly connect them to Keras. We have used the [A detailed example of how to use data generators with Keras](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly)\n",
    "-  Using small class that does simple preprocessing per image gives advantage. This gives easily change something of this phase without producing chaos in the model itself or during data loading.\n",
    "- Using small class that hold parameters that are used or shared between the data loader, the image preprocessor and the baseline model. Passing an instance of this class to them reduced the risk of setting different parameters and obtaining mismatch errors for example during build & compile of the network layers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared Parameter Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParameter:\n",
    "    \n",
    "    def __init__(self, basepath,\n",
    "                 num_classes=28,\n",
    "                 image_rows=512,\n",
    "                 image_cols=512,\n",
    "                 batch_size=200,\n",
    "                 n_channels=1,\n",
    "                 row_scale_factor=4,\n",
    "                 col_scale_factor=4,\n",
    "                 shuffle=False,\n",
    "                 n_epochs=1):\n",
    "        self.basepath = basepath\n",
    "        self.num_classes = num_classes\n",
    "        self.image_rows = image_rows\n",
    "        self.image_cols = image_cols\n",
    "        self.batch_size = batch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.row_scale_factor = row_scale_factor\n",
    "        self.col_scale_factor = col_scale_factor\n",
    "        self.scaled_row_dim = np.int(self.image_rows / self.row_scale_factor)\n",
    "        self.scaled_col_dim = np.int(self.image_cols / self.col_scale_factor)\n",
    "        self.n_epochs = n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = ModelParameter(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Preprocessor\n",
    "- This handles the rescaling of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    \n",
    "    def __init__(self, modelparameter):\n",
    "        self.parameter = modelparameter\n",
    "        self.basepath = self.parameter.basepath\n",
    "        self.scaled_row_dim = self.parameter.scaled_row_dim\n",
    "        self.scaled_col_dim = self.parameter.scaled_col_dim\n",
    "        self.n_channels = self.parameter.n_channels\n",
    "    \n",
    "    def preprocess(self, image):\n",
    "        image = self.resize(image)\n",
    "        image = self.reshape(image)\n",
    "        image = self.normalize(image)\n",
    "        return image\n",
    "    \n",
    "    def resize(self, image):\n",
    "        image = resize(image, (self.scaled_row_dim, self.scaled_col_dim))\n",
    "        return image\n",
    "    \n",
    "    def reshape(self, image):\n",
    "        image = np.reshape(image, (image.shape[0], image.shape[1], self.n_channels))\n",
    "        return image\n",
    "    \n",
    "    def normalize(self, image):\n",
    "        image /= 255 \n",
    "        return image\n",
    "    \n",
    "    def load_image(self, image_id):\n",
    "        image = np.zeros(shape=(512,512,4))\n",
    "        image[:,:,0] = imread(self.basepath + image_id + \"_green\" + \".png\")\n",
    "        image[:,:,1] = imread(self.basepath + image_id + \"_blue\" + \".png\")\n",
    "        image[:,:,2] = imread(self.basepath + image_id + \"_red\" + \".png\")\n",
    "        image[:,:,3] = imread(self.basepath + image_id + \"_yellow\" + \".png\")\n",
    "        return image[:,:,0:self.parameter.n_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ImagePreprocessor(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessed Example Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = images[0,0]\n",
    "preprocessed = preprocessor.preprocess(example)\n",
    "print(example.shape)\n",
    "print(preprocessed.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,10))\n",
    "ax[0].imshow(example, cmap=\"Greens\")\n",
    "ax[1].imshow(preprocessed.reshape(parameter.scaled_row_dim,parameter.scaled_col_dim), cmap=\"Greens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor):\n",
    "        self.current_epoch = 0\n",
    "        self.params = modelparameter\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.dim = (self.params.scaled_row_dim, self.params.scaled_col_dim)\n",
    "        self.batch_size = self.params.batch_size\n",
    "        self.n_channels = self.params.n_channels\n",
    "        self.num_classes = self.params.num_classes\n",
    "        self.shuffle = self.params.shuffle\n",
    "        self.preprocessor = imagepreprocessor\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes, random_state=self.current_epoch)\n",
    "            self.current_epoch += 1\n",
    "    \n",
    "    def get_targets_per_image(self, identifier):\n",
    "        return self.labels.loc[self.labels.Id==identifier].drop(\n",
    "                [\"Id\", \"Target\", \"number_of_targets\"], axis=1).values\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, self.num_classes), dtype=int)\n",
    "        # Generate data\n",
    "        for i, identifier in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            image = self.preprocessor.load_image(identifier)\n",
    "            image = self.preprocessor.preprocess(image)\n",
    "            X[i] = image\n",
    "            # Store class\n",
    "            y[i] = self.get_targets_per_image(identifier)\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictGenerator:\n",
    "    \n",
    "    def __init__(self, predict_Ids, imagepreprocessor, predict_path):\n",
    "        self.preprocessor = imagepreprocessor\n",
    "        self.preprocessor.basepath = predict_path\n",
    "        self.identifiers = predict_Ids\n",
    "    \n",
    "    def predict(self, model):\n",
    "        y = np.empty(shape=(len(self.identifiers), self.preprocessor.parameter.num_classes))\n",
    "        for n in range(len(self.identifiers)):\n",
    "            image = self.preprocessor.load_image(self.identifiers[n])\n",
    "            image = self.preprocessor.preprocess(image)\n",
    "            image = image.reshape((1, *image.shape))\n",
    "            y[n] = model.predict(image)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.initializers import VarianceScaling\n",
    "\n",
    "\n",
    "class BaseLineModel:\n",
    "    \n",
    "    def __init__(self, modelparameter):\n",
    "        self.params = modelparameter\n",
    "        self.num_classes = self.params.num_classes\n",
    "        self.img_rows = self.params.scaled_row_dim\n",
    "        self.img_cols = self.params.scaled_col_dim\n",
    "        self.n_channels = self.params.n_channels\n",
    "        self.input_shape = (self.img_rows, self.img_cols, self.n_channels)\n",
    "        self.my_metrics = ['accuracy']\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape,\n",
    "                             kernel_initializer=VarianceScaling(seed=0)))\n",
    "        self.model.add(Conv2D(32, (3, 3), activation='relu',\n",
    "                             kernel_initializer=VarianceScaling(seed=0)))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Dropout(0.25))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(64, activation='relu',\n",
    "                            kernel_initializer=VarianceScaling(seed=0),))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(self.num_classes, activation='sigmoid'))\n",
    "    \n",
    "    def compile_model(self):\n",
    "        self.model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=self.my_metrics)\n",
    "    \n",
    "    def set_generators(self, train_generator, validation_generator):\n",
    "        self.training_generator = train_generator\n",
    "        self.validation_generator = validation_generator\n",
    "    \n",
    "    def learn(self):\n",
    "        return self.model.fit_generator(generator=self.training_generator,\n",
    "                    validation_data=self.validation_generator,\n",
    "                    epochs=self.params.n_epochs, \n",
    "                    use_multiprocessing=True,\n",
    "                    workers=8)\n",
    "    \n",
    "    def score(self):\n",
    "        return self.model.evaluate_generator(generator=self.validation_generator,\n",
    "                                      use_multiprocessing=True, \n",
    "                                      workers=8)\n",
    "    \n",
    "    def predict(self, predict_generator):\n",
    "        y = predict_generator.predict(self.model)\n",
    "        return y\n",
    "    \n",
    "    def save(self, modeloutputpath):\n",
    "        self.model.save(modeloutputpath)\n",
    "    \n",
    "    def load(self, modelinputpath):\n",
    "        self.model = load_model(modelinputpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model on the first CV-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets\n",
    "partition = partitions[0]\n",
    "labels = train_labels\n",
    "\n",
    "print(\"Number of samples in train: {}\".format(len(partition[\"train\"])))\n",
    "print(\"Number of samples in validation: {}\".format(len(partition[\"validation\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up data generators. Two generators were used, one for training and one for validation. We can see the point of change, when train loss is decreasing and validation loss is increasing. It suffers under overfitting and looses it's ability to make good predictions on the validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(partition['train'], labels, parameter, preprocessor)\n",
    "validation_generator = DataGenerator(partition['validation'], labels, parameter, preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessor = ImagePreprocessor(parameter)\n",
    "submission_predict_generator = PredictGenerator(test_names, test_preprocessor, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run computation and store results as csv\n",
    "target_names = train_labels.drop([\"Target\", \"number_of_targets\", \"Id\"], axis=1).columns\n",
    "\n",
    "if kernelsettings.fit_baseline == True:\n",
    "    model = BaseLineModel(parameter)\n",
    "    model.build_model()\n",
    "    model.compile_model()\n",
    "    model.set_generators(training_generator, validation_generator)\n",
    "    history = model.learn()\n",
    "    \n",
    "    proba_predictions = model.predict(predict_generator)\n",
    "    baseline_proba_predictions = pd.DataFrame(index = partition['validation'],\n",
    "                                              data=proba_predictions,\n",
    "                                              columns=target_names)\n",
    "    baseline_proba_predictions.to_csv(\"baseline_predictions.csv\")\n",
    "    baseline_losses = pd.DataFrame(history.history[\"loss\"], columns=[\"train_loss\"])\n",
    "    baseline_losses[\"val_loss\"] = history.history[\"val_loss\"]\n",
    "    baseline_losses.to_csv(\"baseline_losses.csv\")\n",
    "    \n",
    "    \n",
    "    submission_proba_predictions = model.predict(submission_predict_generator)\n",
    "    baseline_labels = test_labels.copy()\n",
    "    baseline_labels.loc[:, test_labels.drop([\"Id\", \"Target\"], axis=1).columns.values] = submission_proba_predictions\n",
    "    baseline_labels.to_csv(\"baseline_submission_proba.csv\")\n",
    "    \n",
    "else:\n",
    "    baseline_proba_predictions = pd.read_csv(\"G:/DeepLearning/CSE465/input/baseline_predictions.csv\", index_col=0)\n",
    "    baseline_losses = pd.read_csv(\"G:/DeepLearning/CSE465/input/baseline_losses.csv\", index_col=0)\n",
    "    baseline_labels = pd.read_csv(\"G:/DeepLearning/CSE465/input/baseline_submission_proba.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_labels = train_labels.loc[train_labels.Id.isin(partition[\"validation\"])].copy()\n",
    "validation_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_proba_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "y_true = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).values\n",
    "y_pred = np.where(baseline_proba_predictions.values > 0.5, 1, 0)\n",
    "\n",
    "accuracy(y_true.flatten(), y_pred.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it's around 94% percent, this is long way from a good model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_predictions = baseline_proba_predictions.values\n",
    "hot_values = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).values.flatten()\n",
    "one_hot = (hot_values.sum()) / hot_values.shape[0] * 100\n",
    "zero_hot = (hot_values.shape[0] - hot_values.sum()) / hot_values.shape[0] * 100\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,5))\n",
    "sns.distplot(proba_predictions.flatten() * 100, color=\"DodgerBlue\", ax=ax[0])\n",
    "ax[0].set_xlabel(\"Probability in %\")\n",
    "ax[0].set_ylabel(\"Density\")\n",
    "ax[0].set_title(\"Predicted probabilities\")\n",
    "sns.barplot(x=[\"label = 0\", \"label = 1\"], y=[zero_hot, one_hot], ax=ax[1])\n",
    "ax[1].set_ylim([0,100])\n",
    "ax[1].set_title(\"True target label count\")\n",
    "ax[1].set_ylabel(\"Percentage\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "- Model was always very uncertain to predict the presence of a target protein. All probabilities are close to zero and there are only a few with targets where our model predicted a protein structure with higher than 10%.\n",
    "- If we take a look at the true target label count we can see that most of our targets are filled with zero.This corresponds to an absence of corresponding target proteins.\n",
    "- Consequently our high accuracy belongs to the high correct prediction of the absence of target proteins. In contrast we weren't able to predict the presence of a target protein which is the most relevant part! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predictions = np.mean(proba_predictions, axis=0)\n",
    "std_predictions = np.std(proba_predictions, axis=0)\n",
    "mean_targets = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).mean()\n",
    "\n",
    "labels = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).columns.values\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "sns.barplot(x=labels,\n",
    "            y=mean_predictions,\n",
    "            ax=ax[0])\n",
    "ax[0].set_xticklabels(labels=labels,\n",
    "                      rotation=90)\n",
    "ax[0].set_ylabel(\"Mean predicted probability\")\n",
    "ax[0].set_title(\"Mean predicted probability per class over all samples\")\n",
    "sns.barplot(x=labels,\n",
    "           y=std_predictions,\n",
    "           ax=ax[1])\n",
    "ax[1].set_xticklabels(labels=labels,\n",
    "                      rotation=90)\n",
    "ax[1].set_ylabel(\"Standard deviation\")\n",
    "ax[1].set_title(\"Standard deviation of predicted probability per class over all samples\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(20,5))\n",
    "sns.barplot(x=labels, y=mean_targets.values, ax=ax)\n",
    "ax.set_xticklabels(labels=labels,\n",
    "                      rotation=90)\n",
    "ax.set_ylabel(\"Percentage of hot (1)\")\n",
    "ax.set_title(\"Percentage of hot counts (ones) per target class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "- Taking a look at the standard deviation we can see that all samples have nearly the same predicted values. There is no deviation, no difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"Cytosol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "sns.distplot(baseline_proba_predictions[feature].values[0:-10], color=\"Purple\")\n",
    "plt.xlabel(\"Predicted probabilites of {}\".format(feature))\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not seem that the model starts to separate well. The mode is close to the fraction of one-hot-counts over all samples. At least the flat tail gives hope that learning could be in progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wishlist = [\"Nucleoplasm\", \"Cytosol\", \"Plasma membrane\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">def get_targets_per_image(self, identifier):\n",
    "        return self.labels.loc[self.labels.Id==identifier].drop(\n",
    "                [\"Id\", \"Target\", \"number_of_targets\"], axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target List\n",
    "\n",
    "We added a small method DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedDataGenerator(DataGenerator):\n",
    "    \n",
    "    # in contrast to the base DataGenerator we add a target wishlist to init\n",
    "    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor, target_wishlist):\n",
    "        super().__init__(list_IDs, labels, modelparameter, imagepreprocessor)\n",
    "        self.target_wishlist = target_wishlist\n",
    "    \n",
    "    def get_targets_per_image(self, identifier):\n",
    "        return self.labels.loc[self.labels.Id==identifier][self.target_wishlist].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new features and Editing\n",
    "\n",
    "Improving our model adding new features and playing with different parameter settings. \n",
    "Setting the flag improve=True adds a change to our model whereas improve=False uses the old concept we already used in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring Metrics\n",
    "\n",
    "- F1 macro score: [Thanks for the implementation](https://www.kaggle.com/guglielmocamporese/macro-f1-score-keras)\n",
    "- 28 different classes that are very different in their frequency of being present. In addition we have to deal with highly imbalanced classes per single target. Even for the most common target nucleoplasm there are only 40 % of samples that show it and 60 % not. This imbalance becomes even more dramatic for seldom targets like rods and rings. We should attach more importance to true positives. \n",
    "- The mean is not robust towards outliers and consequently not very informative to understand the distribution of f1 scores for each target class.\n",
    "- Adding min and max statistical quantities and standard deviation would have been better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def base_f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return f1\n",
    "\n",
    "def f1_min(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.min(f1)\n",
    "\n",
    "def f1_max(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.max(f1)\n",
    "\n",
    "def f1_mean(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_std(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.std(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Losses and Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_losses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are computing the change of the loss with respect to a change in the weights for each sample one after another. Consequently in original gradient descent we need to pass the whole dataset once for just one single update step of gradient descent. As our initial weights are not sufficent to solve the classification task we need many such update steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackHistory(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedModel(BaseLineModel):\n",
    "    \n",
    "    def __init__(self, modelparameter,\n",
    "                 use_dropout,\n",
    "                 my_metrics=[f1_mean, f1_std, f1_min, f1_max]):\n",
    "        \n",
    "        super().__init__(modelparameter)\n",
    "        self.my_metrics = my_metrics\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "    def learn(self):\n",
    "        self.history = TrackHistory()\n",
    "        return self.model.fit_generator(generator=self.training_generator,\n",
    "                    validation_data=self.validation_generator,\n",
    "                    epochs=self.params.n_epochs, \n",
    "                    use_multiprocessing=True,\n",
    "                    workers=8,\n",
    "                    callbacks = [self.history])\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape,\n",
    "                             kernel_initializer=VarianceScaling(seed=0),))\n",
    "        self.model.add(Conv2D(32, (3, 3), activation='relu',\n",
    "                             kernel_initializer=VarianceScaling(seed=0),))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        if self.use_dropout:\n",
    "            self.model.add(Dropout(0.25))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(64, activation='relu',\n",
    "                            kernel_initializer=VarianceScaling(seed=0),))\n",
    "        if self.use_dropout:\n",
    "            self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(self.num_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = ModelParameter(train_path, num_classes=len(wishlist), n_epochs=100, batch_size=128)\n",
    "preprocessor = ImagePreprocessor(parameter)\n",
    "labels = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = ImprovedDataGenerator(partition['train'], labels,\n",
    "                                           parameter, preprocessor, wishlist)\n",
    "validation_generator = ImprovedDataGenerator(partition['validation'], labels,\n",
    "                                             parameter, preprocessor, wishlist)\n",
    "predict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessor = ImagePreprocessor(parameter)\n",
    "submission_predict_generator = PredictGenerator(test_names, test_preprocessor, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run computation and store results as csv\n",
    "if kernelsettings.fit_improved_baseline == True:\n",
    "    model = ImprovedModel(parameter, use_dropout=use_dropout)\n",
    "    model.build_model()\n",
    "    model.compile_model()\n",
    "    model.set_generators(training_generator, validation_generator)\n",
    "    epoch_history = model.learn()\n",
    "    proba_predictions = model.predict(predict_generator)\n",
    "    #model.save(\"improved_model.h5\")\n",
    "    \n",
    "    improved_proba_predictions = pd.DataFrame(proba_predictions, columns=wishlist)\n",
    "    improved_proba_predictions.to_csv(\"improved_predictions.csv\")\n",
    "    improved_losses = pd.DataFrame(epoch_history.history[\"loss\"], columns=[\"train_loss\"])\n",
    "    improved_losses[\"val_loss\"] = epoch_history.history[\"val_loss\"]\n",
    "    improved_losses.to_csv(\"improved_losses.csv\")\n",
    "    improved_batch_losses = pd.DataFrame(model.history.losses, columns=[\"batch_losses\"])\n",
    "    improved_batch_losses.to_csv(\"improved_batch_losses.csv\")\n",
    "    \n",
    "    improved_submission_proba_predictions = model.predict(submission_predict_generator)\n",
    "    improved_test_labels = test_labels.copy()\n",
    "    improved_test_labels.loc[:, wishlist] = improved_submission_proba_predictions\n",
    "    improved_test_labels.to_csv(\"improved_submission_proba.csv\")\n",
    "# If you already have done a baseline fit once, \n",
    "# you can load predictions as csv and further fitting is not neccessary:\n",
    "else:\n",
    "    improved_proba_predictions = pd.read_csv(\"G:/DeepLearning/CSE465/input/improved_predictions.csv\", index_col=0)\n",
    "    improved_losses= pd.read_csv(\"G:/DeepLearning/CSE465/input/improved_losses.csv\", index_col=0)\n",
    "    improved_batch_losses = pd.read_csv(\"G:/DeepLearning/CSE465/input/improved_batch_losses.csv\", index_col=0)\n",
    "    improved_test_labels = pd.read_csv(\"G:/DeepLearning/CSE465/input/improved_submission_proba.csv\",\n",
    "                                      index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(20,13))\n",
    "ax[0].plot(np.arange(1,6), improved_losses[\"train_loss\"].values, 'r--o', label=\"train_loss\")\n",
    "ax[0].plot(np.arange(1,6), improved_losses[\"val_loss\"].values, 'g--o', label=\"validation_loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_title(\"Loss evolution per epoch\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(improved_batch_losses.batch_losses.values, 'r-+', label=\"train_batch_losses\")\n",
    "ax[1].set_xlabel(\"Number of update steps in total\")\n",
    "ax[1].set_ylabel(\"Train loss\")\n",
    "ax[1].set_title(\"Train loss evolution per batch\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1,figsize=(25,15))\n",
    "sns.distplot(improved_proba_predictions.values[:,0], color=\"Orange\", ax=ax[0])\n",
    "ax[0].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[0]))\n",
    "ax[0].set_xlim([0,1])\n",
    "sns.distplot(improved_proba_predictions.values[:,1], color=\"Purple\", ax=ax[1])\n",
    "ax[1].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[1]))\n",
    "ax[1].set_xlim([0,1])\n",
    "sns.distplot(improved_proba_predictions.values[:,2], color=\"Limegreen\", ax=ax[2])\n",
    "ax[2].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[2]))\n",
    "ax[2].set_xlim([0,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_test_labels.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_proba_predictions.set_index(partition[\"validation\"], inplace=True)\n",
    "improved_proba_predictions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_labels.set_index(\"Id\", inplace=True)\n",
    "validation_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1,figsize=(20,18))\n",
    "for n in range(len(wishlist)):\n",
    "    sns.distplot(improved_proba_predictions.loc[validation_labels[wishlist[n]] == 1,\n",
    "                                                wishlist[n]], color=\"Green\", label=\"1-hot\", ax=ax[n])\n",
    "    sns.distplot(improved_proba_predictions.loc[validation_labels[wishlist[n]] == 0,\n",
    "                                                wishlist[n]], color=\"Red\", label=\"0-zero\", ax=ax[n])\n",
    "    ax[n].set_title(wishlist[n])\n",
    "    ax[n].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_cytosol = 0.4\n",
    "th_plasma_membrane = 0.2\n",
    "th_nucleoplasm = 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_submission = improved_test_labels.copy()\n",
    "improved_submission.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_submission[\"Nucleoplasm\"] = np.where(improved_test_labels[\"Nucleoplasm\"] >= th_nucleoplasm, 1, 0)\n",
    "improved_submission[\"Cytosol\"] = np.where(improved_test_labels[\"Cytosol\"] >= th_cytosol, 1, 0)\n",
    "improved_submission[\"Plasma membrane\"] = np.where(\n",
    "    improved_test_labels[\"Plasma membrane\"] >= th_plasma_membrane, 1, 0)\n",
    "\n",
    "improved_submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_target(row):\n",
    "    target_list = []\n",
    "    for col in validation_labels.drop([\"Target\", \"number_of_targets\"], axis=1).columns:\n",
    "        if row[col] == 1:\n",
    "            target_list.append(str(reverse_train_labels[col]))\n",
    "    if len(target_list) == 0:\n",
    "        return str(0)\n",
    "    return \" \".join(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_submission[\"Predicted\"] = improved_submission.apply(lambda l: transform_to_target(l), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = improved_submission.loc[:, [\"Id\", \"Predicted\"]]\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"improved_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = ModelParameter(train_path, num_classes=len(wishlist), n_epochs=500, batch_size=2000)\n",
    "preprocessor = ImagePreprocessor(parameter)\n",
    "labels = train_labels\n",
    "\n",
    "training_generator = ImprovedDataGenerator(partition['train'], labels,\n",
    "                                           parameter, preprocessor, wishlist)\n",
    "validation_generator = ImprovedDataGenerator(partition['validation'], labels,\n",
    "                                             parameter, preprocessor, wishlist)\n",
    "predict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kernelsettings.fit_improved_higher_batchsize == True:\n",
    "    model = ImprovedModel(parameter, use_dropout=True)\n",
    "    model.build_model()\n",
    "    model.compile_model()\n",
    "    model.set_generators(training_generator, validation_generator)\n",
    "    epoch_history = model.learn()\n",
    "    proba_predictions = model.predict(predict_generator)\n",
    "    #model.save(\"improved_model.h5\")\n",
    "    improved_proba_predictions = pd.DataFrame(proba_predictions, columns=wishlist)\n",
    "    improved_proba_predictions.to_csv(\"improved_hbatch_predictions.csv\")\n",
    "    improved_losses = pd.DataFrame(epoch_history.history[\"loss\"], columns=[\"train_loss\"])\n",
    "    improved_losses[\"val_loss\"] = epoch_history.history[\"val_loss\"]\n",
    "    improved_losses.to_csv(\"improved_hbatch_losses.csv\")\n",
    "    improved_batch_losses = pd.DataFrame(model.history.losses, columns=[\"batch_losses\"])\n",
    "    improved_batch_losses.to_csv(\"improved_hbatch_batch_losses.csv\")\n",
    "\n",
    "else:\n",
    "    improved_proba_predictions = pd.read_csv(\n",
    "        \"G:/DeepLearning/CSE465/input/improved_hbatch_predictions.csv\", index_col=0)\n",
    "    improved_losses= pd.read_csv(\n",
    "        \"G:/DeepLearning/CSE465/input/improved_hbatch_losses.csv\", index_col=0)\n",
    "    improved_batch_losses = pd.read_csv(\"G:/DeepLearning/CSE465/input/improved_hbatch_batch_losses.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(20,13))\n",
    "ax[0].plot(np.arange(1,11), improved_losses[\"train_loss\"].values, 'r--o', label=\"train_loss\")\n",
    "ax[0].plot(np.arange(1,11), improved_losses[\"val_loss\"].values, 'g--o', label=\"validation_loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_title(\"Loss evolution per epoch\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(improved_batch_losses.batch_losses.values, 'r-+', label=\"train_batch_losses\")\n",
    "ax[1].set_xlabel(\"Number of update steps in total\")\n",
    "ax[1].set_ylabel(\"Train loss\")\n",
    "ax[1].set_title(\"Train loss evolution per batch\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1,figsize=(25,15))\n",
    "sns.distplot(improved_proba_predictions.values[:,0], color=\"Orange\", ax=ax[0])\n",
    "ax[0].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[0]))\n",
    "ax[0].set_xlim([0,1])\n",
    "sns.distplot(improved_proba_predictions.values[:,1], color=\"Purple\", ax=ax[1])\n",
    "ax[1].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[1]))\n",
    "ax[1].set_xlim([0,1])\n",
    "sns.distplot(improved_proba_predictions.values[:,2], color=\"Limegreen\", ax=ax[2])\n",
    "ax[2].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[2]))\n",
    "ax[2].set_xlim([0,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run computation and store results as csv\n",
    "if kernelsettings.fit_improved_without_dropout == True:\n",
    "    model = ImprovedModel(parameter, use_dropout=False)\n",
    "    model.build_model()\n",
    "    model.compile_model()\n",
    "    model.set_generators(training_generator, validation_generator)\n",
    "    epoch_history = model.learn()\n",
    "    proba_predictions = model.predict(predict_generator)\n",
    "    #model.save(\"improved_model.h5\")\n",
    "    improved_proba_predictions = pd.DataFrame(proba_predictions, columns=wishlist)\n",
    "    improved_proba_predictions.to_csv(\"improved_nodropout_predictions.csv\")\n",
    "    improved_losses = pd.DataFrame(epoch_history.history[\"loss\"], columns=[\"train_loss\"])\n",
    "    improved_losses[\"val_loss\"] = epoch_history.history[\"val_loss\"]\n",
    "    improved_losses.to_csv(\"improved_nodropout_losses.csv\")\n",
    "    improved_batch_losses = pd.DataFrame(model.history.losses, columns=[\"batch_losses\"])\n",
    "    improved_batch_losses.to_csv(\"improved_nodropout_batch_losses.csv\")\n",
    "# If you already have done a baseline fit once, \n",
    "# you can load predictions as csv and further fitting is not neccessary:\n",
    "else:\n",
    "    improved_proba_predictions_no_dropout = pd.read_csv(\n",
    "        \"G:/DeepLearning/CSE465/input/improved_nodropout_predictions.csv\", index_col=0)\n",
    "    improved_losses_no_dropout= pd.read_csv(\n",
    "        \"G:/DeepLearning/CSE465/input/improved_nodropout_losses.csv\", index_col=0)\n",
    "    improved_batch_losses_no_dropout = pd.read_csv(\n",
    "        \"G:/DeepLearning/CSE465/input/improved_nodropout_batch_losses.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(20,13))\n",
    "ax[0].plot(np.arange(1,11), improved_losses[\"train_loss\"].values, 'r--o', label=\"train_loss_dropout\")\n",
    "ax[0].plot(np.arange(1,11), improved_losses_no_dropout[\"train_loss\"].values, 'r-o', label=\"train_loss_no_dropout\")\n",
    "ax[0].plot(np.arange(1,11), improved_losses[\"val_loss\"].values, 'g--o', label=\"validation_loss\")\n",
    "ax[0].plot(np.arange(1,11), improved_losses_no_dropout[\"val_loss\"].values, 'g-o', label=\"validation_loss_no_dropout\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_title(\"Loss evolution per epoch\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(improved_batch_losses.batch_losses.values[-800::], 'r-+', label=\"train_batch_losses_dropout\")\n",
    "ax[1].plot(improved_batch_losses_no_dropout.batch_losses.values[-800::], 'b-+',\n",
    "           label=\"train_batch_losses_no_dropout\")\n",
    "ax[1].set_xlabel(\"Number of update steps in total\")\n",
    "ax[1].set_ylabel(\"Train loss\")\n",
    "ax[1].set_title(\"Train loss evolution per batch\");\n",
    "ax[1].legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "hpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
